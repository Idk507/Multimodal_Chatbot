{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# keys for the services we will use\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyDaNJU8ZovOI1TijoFyao2iZbVTj7NM9zg\"\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import base64\n",
    "import pytesseract\n",
    "import fitz  # PyMuPDF\n",
    "import camelot\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema.document import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dhanu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file_path =  \\'attention.pdf\\'\\n\\n# Reference: https://docs.unstructured.io/open-source/core-functionality/chunking\\nchunks = partition_pdf(\\n    filename=file_path,\\n    infer_table_structure=True,            # extract tables\\n    strategy=\"hi_res\",                     # mandatory to infer tables\\n\\n    extract_image_block_types=[\"Image\"],   # Add \\'Table\\' to list to extract image of tables\\n    # image_output_dir_path=output_path,   # if None, images and tables will saved in base64\\n\\n    extract_image_block_to_payload=True,   # if true, will extract base64 for API usage\\n\\n    chunking_strategy=\"by_title\",          # or \\'basic\\'\\n    max_characters=10000,                  # defaults to 500\\n    combine_text_under_n_chars=2000,       # defaults to 0\\n    new_after_n_chars=6000,\\n\\n    # extract_images_in_pdf=True,          # deprecated\\n)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"file_path =  'attention.pdf'\n",
    "\n",
    "# Reference: https://docs.unstructured.io/open-source/core-functionality/chunking\n",
    "chunks = partition_pdf(\n",
    "    filename=file_path,\n",
    "    infer_table_structure=True,            # extract tables\n",
    "    strategy=\"hi_res\",                     # mandatory to infer tables\n",
    "\n",
    "    extract_image_block_types=[\"Image\"],   # Add 'Table' to list to extract image of tables\n",
    "    # image_output_dir_path=output_path,   # if None, images and tables will saved in base64\n",
    "\n",
    "    extract_image_block_to_payload=True,   # if true, will extract base64 for API usage\n",
    "\n",
    "    chunking_strategy=\"by_title\",          # or 'basic'\n",
    "    max_characters=10000,                  # defaults to 500\n",
    "    combine_text_under_n_chars=2000,       # defaults to 0\n",
    "    new_after_n_chars=6000,\n",
    "\n",
    "    # extract_images_in_pdf=True,          # deprecated\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "import pandas as pd\n",
    "from langchain_community.embeddings import  HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.generativeai import GenerativeModel \n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import  GoogleGenerativeAI,GoogleGenerativeAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyBYveb08XSjbJbCgVC2AxYegxwz1nuu3Fw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-exp-1206\n",
      "models/gemini-exp-1121\n",
      "models/gemini-exp-1114\n",
      "models/learnlm-1.5-pro-experimental\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "for model in genai.list_models():\n",
    "    if 'generateContent' in model.supported_generation_methods:\n",
    "        print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalPDFExtractor:\n",
    "    def __init__(self,file_path,output_path=\"content\"):\n",
    "        self.file_path = file_path\n",
    "        self.output_path = output_path \n",
    "        os.makedirs(output_path,exist_ok=True)\n",
    "\n",
    "        #intialize models \n",
    "        self.text_summarization_model = ChatGroq( \n",
    "           temperature=0.1,model = \"llama-3.1-8b-instant\",http_client=None  )\n",
    "        \n",
    "        #self.text_summarization_model = genai.GenerativeModel(\"models/gemini-pro\")\n",
    "    \n",
    "\n",
    "        self.gemini_model = genai.GenerativeModel(\"models/gemini-1.5-flash\")\n",
    "        self.gemini_text_model = genai.GenerativeModel(\"models/gemini-pro\")\n",
    "    \n",
    "    def extract_pdf_content(self):\n",
    "        doc = fitz.open(self.file_path)\n",
    "        texts, images, tables = [], [], []\n",
    "\n",
    "        for page_num, page in enumerate(doc):\n",
    "            # Extract text\n",
    "            page_text = page.get_text()\n",
    "            texts.append({\n",
    "                \"text\": page_text,\n",
    "                \"page_number\": page_num + 1\n",
    "            })\n",
    "            \n",
    "            # Image Extraction\n",
    "            image_list = page.get_images(full=True)\n",
    "            \n",
    "            for img_index, img in enumerate(image_list):\n",
    "                xref = img[0]\n",
    "                base_image = doc.extract_image(xref)\n",
    "                \n",
    "                # Get image details\n",
    "                image_byte = base_image['image']\n",
    "                image_ext = base_image.get('ext', 'png').lower()\n",
    "                \n",
    "                # Generate unique filename\n",
    "                img_path = os.path.join(self.output_path, f'page_{page_num+1}_img_{img_index}.{image_ext}')\n",
    "                \n",
    "                # Save image file\n",
    "                with open(img_path, 'wb') as f:\n",
    "                    f.write(image_byte)\n",
    "                \n",
    "                # Encode image to base64\n",
    "                base64_image = base64.b64encode(image_byte).decode('utf-8')\n",
    "                \n",
    "                # Collect image metadata\n",
    "                images.append({\n",
    "                    \"image_path\": img_path,\n",
    "                    \"base64_image\": base64_image,\n",
    "                    \"page_number\": page_num + 1,\n",
    "                    \"image_extension\": image_ext\n",
    "                })\n",
    "            \n",
    "            # Table Extraction\n",
    "            tabula_tables = tabula.read_pdf(self.file_path, pages=page_num+1, multiple_tables=True)\n",
    "            for table in tabula_tables:\n",
    "                tables.append({\n",
    "                    'html': table.to_html(),\n",
    "                    'dataframe': table\n",
    "                })\n",
    "        \n",
    "        # Close the document\n",
    "        doc.close()\n",
    "        \n",
    "        return {\n",
    "            'texts': texts,\n",
    "            'images': images,\n",
    "            'tables': tables\n",
    "        }\n",
    "    \n",
    "\n",
    "    def _summarize_images(self, images):\n",
    "        summaries = []\n",
    "        \n",
    "        # Ensure images are present\n",
    "        if not images:\n",
    "            print(\"No images found to summarize.\")\n",
    "            return summaries\n",
    "        \n",
    "        for image in images:\n",
    "            # Validate image data\n",
    "            if not image or 'base64_image' not in image:\n",
    "                print(\"Invalid image data, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Decode base64 image\n",
    "            image_data = base64.b64decode(image['base64_image'])\n",
    "            \n",
    "            # Prepare image for Gemini\n",
    "            img_input = {\n",
    "                'mime_type': 'image/png',\n",
    "                'data': image_data\n",
    "            }\n",
    "            \n",
    "            # Detailed prompt for image analysis\n",
    "            prompt = \"\"\"Analyze this image comprehensively:\n",
    "            - Describe the main visual elements\n",
    "            - Identify any key components, diagrams, or graphs\n",
    "            - Explain the context and purpose of the image\n",
    "            - Provide technical insights if applicable\n",
    "            \n",
    "            Ensure your summary is detailed and informative.\"\"\"\n",
    "            \n",
    "            # Generate summary using Gemini\n",
    "            response = self.gemini_model.generate_content([prompt, img_input])\n",
    "            \n",
    "            # Validate and store summary\n",
    "            if response and response.text:\n",
    "                summaries.append(response.text)\n",
    "            else:\n",
    "                summaries.append(\"Could not generate a summary for this image.\")\n",
    "        \n",
    "            return summaries\n",
    "    def _summarize_texts(self, texts):\n",
    "        summaries = []\n",
    "        for text in texts:\n",
    "            prompt = f\"\"\"Summarize the following text chunk concisely. \n",
    "            Focus on the key information and main points.\n",
    "            \n",
    "            Text chunk: {text['text']}\"\"\"\n",
    "            \n",
    "            response = self.gemini_text_model.generate_content(prompt)\n",
    "            summaries.append(response.text)\n",
    "        \n",
    "        return summaries\n",
    "        \n",
    "    def _summarize_tables(self, tables):\n",
    "        summaries = []\n",
    "        for table in tables:\n",
    "            prompt = f\"\"\"Analyze this table and provide a concise summary. \n",
    "            Highlight the key insights and main takeaways.\n",
    "            \n",
    "            Table data: {table['html']}\"\"\"\n",
    "            \n",
    "            response = self.gemini_text_model.generate_content(prompt)\n",
    "            summaries.append(response.text)\n",
    "        \n",
    "        return summaries\n",
    "    \n",
    "    def summarize_content(self, extracted_content):\n",
    "    # Summarize text content\n",
    "        text_summaries = self._summarize_texts(extracted_content.get('texts', []))\n",
    "        \n",
    "        # Summarize table content\n",
    "        table_summaries = self._summarize_tables(extracted_content.get('tables', []))\n",
    "        \n",
    "        # Summarize image content\n",
    "        image_summaries = self._summarize_images(extracted_content.get('images', []))\n",
    "        \n",
    "        return {\n",
    "            'text_summaries': text_summaries,\n",
    "            'table_summaries': table_summaries,\n",
    "            'image_summaries': image_summaries\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def create_multi_vector_retriever(self, summaries):\n",
    "        # Use Google Generative AI Embeddings\n",
    "        vectorstore = Chroma(\n",
    "            collection_name=\"multi_modal_rag\", \n",
    "            embedding_function= HuggingFaceBgeEmbeddings()\n",
    "        )\n",
    "        store = InMemoryStore()\n",
    "        id_key = \"doc_id\"\n",
    "        \n",
    "        retriever = MultiVectorRetriever(\n",
    "            vectorstore=vectorstore,\n",
    "            docstore=store,\n",
    "            id_key=id_key,\n",
    "        )\n",
    "        \n",
    "        # Add texts\n",
    "        if summaries['text_summaries']:\n",
    "            text_doc_ids = [str(uuid.uuid4()) for _ in summaries['text_summaries']]\n",
    "            text_summary_docs = [\n",
    "                Document(\n",
    "                    page_content=summary, \n",
    "                    metadata={id_key: text_doc_ids[i]}\n",
    "                ) for i, summary in enumerate(summaries['text_summaries'])\n",
    "            ]\n",
    "            retriever.vectorstore.add_documents(text_summary_docs)\n",
    "        \n",
    "        # Add table summaries\n",
    "        if summaries['table_summaries']:\n",
    "            table_doc_ids = [str(uuid.uuid4()) for _ in summaries['table_summaries']]\n",
    "            table_summary_docs = [\n",
    "                Document(\n",
    "                    page_content=summary, \n",
    "                    metadata={id_key: table_doc_ids[i]}\n",
    "                ) for i, summary in enumerate(summaries['table_summaries'])\n",
    "            ]\n",
    "            retriever.vectorstore.add_documents(table_summary_docs)\n",
    "        \n",
    "        # Add image summaries\n",
    "        if summaries['image_summaries']:\n",
    "            img_doc_ids = [str(uuid.uuid4()) for _ in summaries['image_summaries']]\n",
    "            img_summary_docs = [\n",
    "                Document(\n",
    "                    page_content=summary, \n",
    "                    metadata={id_key: img_doc_ids[i]}\n",
    "                ) for i, summary in enumerate(summaries['image_summaries'])\n",
    "            ]\n",
    "            retriever.vectorstore.add_documents(img_summary_docs)\n",
    "        \n",
    "        return retriever\n",
    "    \n",
    "    def build_rag_chain(self, retriever):\n",
    "        def parse_docs(docs):\n",
    "            b64 = []\n",
    "            text = []\n",
    "            for doc in docs:\n",
    "                try:\n",
    "                    base64.b64decode(doc)\n",
    "                    b64.append(doc)\n",
    "                except Exception:\n",
    "                    text.append(doc)\n",
    "            return {\"images\": b64, \"texts\": text}\n",
    "        \n",
    "        def retrieve_and_generate(question):\n",
    "            # Retrieve relevant documents\n",
    "            retrieved_docs = retriever.invoke(question)\n",
    "            \n",
    "            # Prepare context\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "            \n",
    "            # Generate response using Gemini\n",
    "            full_prompt = f\"\"\"Based on the following context, answer the question:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a detailed and accurate response.\"\"\"\n",
    "            \n",
    "            response = self.gemini_text_model.generate_content(full_prompt)\n",
    "            return response.text\n",
    "\n",
    "        return retrieve_and_generate\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"attention.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize extractor\n",
    "extractor = MultiModalPDFExtractor(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: Dec 13, 2024 12:27:40 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\n",
      "WARNING: No Unicode mapping for summationtext (80) in font THPNLT+CMEX9\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images extracted: 3\n",
      "Image path: content\\page_3_img_0.png, Base64 length: 161660\n",
      "Image path: content\\page_4_img_0.png, Base64 length: 25620\n",
      "Image path: content\\page_4_img_1.png, Base64 length: 57988\n"
     ]
    }
   ],
   "source": [
    "# After extract_pdf_content()\n",
    "extracted_content = extractor.extract_pdf_content()\n",
    "print(f\"Total images extracted: {len(extracted_content['images'])}\")\n",
    "for img in extracted_content['images']:\n",
    "    print(f\"Image path: {img['image_path']}, Base64 length: {len(img['base64_image'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Resource has been exhausted (e.g. check quota).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m summaries \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummarize_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted_content\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[65], line 149\u001b[0m, in \u001b[0;36mMultiModalPDFExtractor.summarize_content\u001b[1;34m(self, extracted_content)\u001b[0m\n\u001b[0;32m    146\u001b[0m text_summaries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_texts(extracted_content\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexts\u001b[39m\u001b[38;5;124m'\u001b[39m, []))\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Summarize table content\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m table_summaries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_summarize_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted_content\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtables\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Summarize image content\u001b[39;00m\n\u001b[0;32m    152\u001b[0m image_summaries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_images(extracted_content\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m, []))\n",
      "Cell \u001b[1;32mIn[65], line 139\u001b[0m, in \u001b[0;36mMultiModalPDFExtractor._summarize_tables\u001b[1;34m(self, tables)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n\u001b[0;32m    134\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mAnalyze this table and provide a concise summary. \u001b[39m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124m    Highlight the key insights and main takeaways.\u001b[39m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124m    \u001b[39m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124m    Table data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m--> 139\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemini_text_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     summaries\u001b[38;5;241m.\u001b[39mappend(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m summaries\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[0;32m    332\u001b[0m             request,\n\u001b[0;32m    333\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_options,\n\u001b[0;32m    334\u001b[0m         )\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:830\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 830\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[0;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    208\u001b[0m         error_list,\n\u001b[0;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    210\u001b[0m         original_timeout,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\google\\api_core\\timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dhanu\\.conda\\envs\\idk_gpu\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mResourceExhausted\u001b[0m: 429 Resource has been exhausted (e.g. check quota)."
     ]
    }
   ],
   "source": [
    "\n",
    "summaries = extractor.summarize_content(extracted_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image summaries generated: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Image summaries generated: {len(summaries['image_summaries'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_summaries': ['Google grants permission to reproduce the tables and figures in the \"Attention Is All You Need\" paper by Vaswani et al. for use in journalistic or scholarly works with proper attribution. The paper introduces the Transformer model, an attention-based network architecture for sequence transduction tasks, which outperforms existing models on machine translation and constituency parsing tasks. The codebase for the Transformer model is publicly available as part of Google\\'s tensor2tensor library.',\n",
       "  '**Key Information:**\\n\\n* Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), have been the standard approach for sequence modeling.\\n* RNNs suffer from sequential computation, limiting parallelization and efficiency.\\n* Attention mechanisms enhance sequence models by capturing dependencies beyond sequential order.\\n* The Transformer model introduces an entirely attention-based architecture, eliminating sequential computation.\\n\\n**Main Points:**\\n\\n* **Attention Mechanisms:** Attention mechanisms overcome the limitations of sequential computation by allowing global dependencies.\\n* **Transformer Architecture:** The Transformer uses solely attention mechanisms to build encoder and decoder representations.\\n* **Advantages:** The Transformer enables massively parallelization and outperforms RNN-based models in translation quality.\\n* **Comparison to Other Models:** The Transformer differs from models like Extended Neural GPU, ByteNet, and ConvS2S in its constant computation cost for distant dependencies. It also utilizes self-attention, which has proven successful in other tasks.',\n",
       "  '**Transformer Architecture:**\\n\\n- **Encoder and Decoder:**\\n    - Stacks of identical layers\\n    - Encoder: Self-attention and feed-forward layers\\n    - Decoder: Encoder attention, self-attention, and feed-forward layers\\n- **Attention:**\\n    - Weights inputs based on query, keys, and values\\n    - Decoder self-attention masked to prevent future position attention\\n    - Layer normalization and residual connections used throughout the model',\n",
       "  'Scaled Dot-Product Attention:\\n- Calculates attention weights between queries and keys, using dot products scaled by √dk.\\n- Weights are applied to values to generate output.\\n\\nMulti-Head Attention:\\n- Splits queries, keys, and values into multiple parallel projections.\\n- Performs multiple Scaled Dot-Product Attention operations in parallel.',\n",
       "  '**Multi-Head Attention:**\\n\\n* Allows attention to different representation subspaces and positions.\\n* Concatenates multiple attention heads for final output.\\n\\n**Transformer Attention Types:**\\n\\n* Encoder-decoder attention: Decoder accesses encoder outputs.\\n* Encoder self-attention: Encoder positions attend to previous encoder layer.\\n* Decoder self-attention: Decoder positions attend to previous decoder positions, with leftward masking to preserve auto-regressive property.\\n\\n**Position-wise Feed-Forward Networks:**\\n\\n* Fully connected feed-forward networks applied to each position independently.\\n* Consists of two linear transformations with ReLU activation.\\n\\n**Embeddings and Softmax:**\\n\\n* Input and output tokens embedded into vectors using learned weights.\\n* Decoder output converted to next-token probabilities via linear transformation and softmax.\\n* Embedding and pre-softmax weights are shared and scaled by √dmodel.',\n",
       "  '**Table 1:**\\n\\n| Layer Type | Complexity per Layer | Sequential Operations | Maximum Path Length |\\n|---|---|---|---|\\n| Self-Attention | O(n^2 * d) | O(1) | O(1) |\\n| Recurrent | O(n * d^2) | O(n) | O(n) |\\n| Convolutional | O(k * n * d^2) | O(1) | O(logk(n)) |\\n| Self-Attention (restricted) | O(r * n * d) | O(1) | O(n/r) |\\n\\n**Positional Encoding:**\\n\\n* Injects information about token position into the model.\\n* Implemented using sine and cosine functions of different frequencies.\\n* Allows the model to learn long-range dependencies over relative positions.\\n\\n**Self-Attention Advantages:**\\n\\n* **Low computational complexity:** O(n^2 * d) per layer.\\n* **High parallelization:** O(1) sequential operations.\\n* **Shorter path lengths:** Enables learning of long-range dependencies.',\n",
       "  '**Key Information**\\n\\n* Self-attention outperforms convolution for long sequences, as it reduces the maximum path length to O(n/r).\\n* Separable convolutions approach the efficiency of self-attention, but still may be more expensive.\\n* Self-attention can provide interpretable models by learning syntactic and semantic sentence structures.\\n\\n**Training Details**\\n\\n* Models trained on WMT 2014 English-German and English-French datasets.\\n* Hardware: 8 NVIDIA P100 GPUs.\\n* Training time: 12 hours (base models), 3.5 days (big models).\\n* Adam optimizer with variable learning rate.\\n* Regularization: dropout, label smoothing, weight decay.',\n",
       "  '**Key Information:**\\n\\n* The Transformer outperforms previous models in English-to-German and English-to-French translation tasks.\\n* The Transformer model significantly reduces training costs while achieving higher BLEU scores.\\n\\n**Main Points:**\\n\\n* The Transformer model with the \"big\" configuration achieves a BLEU score of 28.4 on English-to-German translation, surpassing previous state-of-the-art by over 2.0.\\n* Even the base model surpasses competitors at a fraction of the training cost.\\n* On English-to-French translation, the Transformer\\'s \"big\" model achieves a BLEU score of 41.8, requiring less than 1/4 the training cost of previous state-of-the-art models.\\n* The Transformer incorporates techniques like residual dropout, label smoothing, and beam search to enhance performance.',\n",
       "  '**Table 3: Transformer Architecture Variations**\\n\\n**Key Findings:**\\n\\n* **Attention heads and dimensions:** Optimal setting has 8 heads, 64 dimension attention keys/values.\\n* **Attention key size (dk):** Smaller values hurt performance, suggesting improved compatibility functions may be beneficial.\\n* **Model size and dropout:** Larger models and dropout regularization improve performance.\\n* **Positional encoding:** Learned positional embeddings perform similarly to sinusoidal encoding.\\n* **English constituency parsing:** Transformer model achieves strong results on this task, outperforming RNN sequence-to-sequence models.',\n",
       "  '**Key Information and Main Points:**\\n\\n* The Transformer performs well in English constituency parsing, achieving a competitive F1 score of 92.7 on WSJ Section 23.\\n* The Transformer outperforms previous models, including the Berkeley-Parser, even when trained solely on WSJ data.\\n* The Transformer architecture relies on attention mechanisms and replaces recurrent layers in encoder-decoder models.\\n* The Transformer shows significant performance gains in translation tasks, outperforming ensembles of previously reported models.\\n* Researchers plan to extend the Transformer to other modalities (e.g., images, audio) and explore non-sequential generation techniques.',\n",
       "  '**Research in Neural Networks:**\\n\\nThis text presents several prominent research papers on neural networks, including:\\n\\n* **RNN Encoder-Decoder for Machine Translation:** (Cho et al., [5]) Introduces a method for learning phrase representations using RNNs.\\n* **Xception Convolutional Network:** (Chollet, [6]) Develops a deep learning model with depthwise separable convolutions.\\n* **Empirical Evaluation of Gated RNNs:** (Chung et al., [7]) Assesses the performance of gated recurrent neural networks for sequence modeling.\\n* **Recurrent Neural Network Grammars:** (Dyer et al., [8]) Explores the use of RNNs in natural language grammar modeling.\\n* **Convolutional Sequence-to-Sequence Learning:** (Gehring et al., [9]) Utilizes convolutional neural networks for sequence modeling tasks.\\n* **Long Short-Term Memory:** (Hochreiter and Schmidhuber, [13]) Presents a type of RNN designed for long-term dependency learning.\\n* **Deep Residual Learning:** (He et al., [11]) Proposes a deep learning architecture with residual connections.\\n* **Multi-Task Sequence-to-Sequence Learning:** (Luong et al., [23]) Investigates the use of RNNs for multiple related tasks.\\n* **Attention-Based Neural Machine Translation:** (Luong et al., [24]) Introduces attention mechanisms for improving neural machine translation.',\n",
       "  \"**Key Information:**\\n\\nThis text chunk presents references for research in natural language processing (NLP), including:\\n\\n* Annotated corpora building (Mitchell et al., 1993)\\n* Effective self-training for parsing (McClosky et al., 2006)\\n* Attentive language models (Parikh et al., 2016)\\n* Deep reinforced abstractive summarization (Paulus et al., 2017)\\n* Accurate and interpretable tree annotation (Petrov et al., 2006)\\n* Enhancing language models with output embeddings (Press and Wolf, 2016)\\n* Neural machine translation for rare words (Sennrich et al., 2015)\\n* Mixture-of-experts layer for very large neural networks (Shazeer et al., 2017)\\n* Dropout for preventing overfitting (Srivastava et al., 2014)\\n* End-to-end memory networks (Sukhbaatar et al., 2015)\\n* Sequence-to-sequence learning (Sutskever et al., 2014)\\n* Inception architecture for computer vision (Szegedy et al., 2015)\\n* Grammar as a foreign language (Vinyals et al., 2015)\\n* Google's neural machine translation system (Wu et al., 2016)\\n* Fast-forward connections for neural machine translation (Zhou et al., 2016)\\n* Fast and accurate shift-reduce constituent parsing (Zhu et al., 2013)\",\n",
       "  'Attention visualizations reveal that attention heads in neural networks can follow long-distance dependencies. In Figure 3, many attention heads attend to the distant word \"making\" to complete the phrase \"making...more difficult.\"',\n",
       "  \"Despite the inherent imperfections of laws, their application should be fair and just. In the author's view, this aspect is currently lacking.\",\n",
       "  'The law may not be perfect, but its application should be just. Currently, the fair application of the law is lacking.'],\n",
       " 'table_summaries': ['**Summary:**\\n\\nThe table provides information on the complexity, sequential nature, and maximum path length of different neural network layer types.\\n\\n**Key Insights:**\\n\\n* **Self-attention layers** have the highest complexity (O(n^2 · d)), but they are non-sequential and have a constant maximum path length.\\n* **Recurrent layers** have a medium complexity (O(n · d^2)) and are sequential, meaning they process data in a specific order. They also have a maximum path length equal to the sequence length (O(n)).\\n* **Convolutional layers** have a lower complexity (O(k · n · d^2)) compared to self-attention and recurrent layers. They are non-sequential and have a maximum path length that is logarithmic with respect to the kernel size (O(logk(n))).\\n* **Restricted self-attention layers** have a complexity that scales linearly with the number of features (O(r · n · d)) and a maximum path length that is inversely proportional to the restriction factor (O(n/r)).\\n\\n**Main Takeaways:**\\n\\n* The choice of layer type depends on the desired trade-off between complexity, sequential nature, and maximum path length.\\n* Self-attention layers are suitable for long-range dependencies and non-sequential data.\\n* Recurrent layers are suitable for sequential data and tasks involving memory.\\n* Convolutional layers are suitable for local dependencies and data with spatial relationships.\\n* Restricted self-attention layers offer a compromise between complexity and scalability.',\n",
       "  \"**Key Insights:**\\n\\n* The model was designed to learn attention mechanisms effectively by using relative positional embeddings (PEpos), which allow for representing offsets in the input linearly.\\n* Sinusoidal positional embeddings were chosen over learned positional embeddings due to their comparable performance in experiments.\\n\\n**Main Takeaways:**\\n\\n* The design of the model's positional embeddings emphasizes the importance of attention mechanisms in capturing positional information in the input data.\\n* The effectiveness of sinusoidal positional embeddings suggests that fixed, linear relationships between embeddings at different positions are sufficient for the model's attention mechanisms to learn effectively.\",\n",
       "  '## Summary of BLEU Scores and Training Costs\\n\\nThis table compares the BLEU scores and training costs (FLOPs) for various machine translation models.\\n\\n## Key Insights\\n\\n* **Deep-Att + PosUnk Ensemble** achieves the highest BLEU score (40.4) but has a high training cost (8.0 · 1020 FLOPs).\\n* **GNMT + RL Ensemble** provides a good balance between BLEU score (41.16) and training cost (1.8 · 1020 FLOPs).\\n* **ConvS2S Ensemble** also offers a compromise, with a BLEU score of 41.29 and a training cost of 7.7 · 1019 FLOPs.\\n\\n## Main Takeaways\\n\\n* Ensembles of models generally achieve higher BLEU scores than single models.\\n* Higher training costs are typically associated with improved BLEU performance.\\n* Researchers should consider the trade-off between BLEU score and training cost when selecting a model for machine translation.',\n",
       "  '**Summary:**\\n\\nThe table presents a series of hyperparameter configurations and their corresponding results for training a neural machine translation (NMT) model. Each row represents a specific configuration, and the columns show the PPL (perplexity) and BLEU scores on the development set, along with other details such as the number of training steps and model size.\\n\\n**Key Insights:**\\n\\n* **Impact of Hidden Network Size:** Increasing the hidden dimension size (dmodel) and feedforward dimension size (dff) generally leads to better results, but at the cost of increased model size and training time.\\n* **Importance of Attention Layer:** Using a multi-head attention mechanism with a higher number of heads (h) improves both PPL and BLEU scores.\\n* **Optimal Kernel Sizes:** For the convolutional layers (dk and dv), smaller kernel sizes (16 or 32) perform better than larger sizes.\\n* **Regularization Parameters:** Applying dropout (Pdrop) and label smoothing (εls) helps prevent overfitting and improves generalization.\\n* **Embedding Type:** Replacing sinusoidal positional embeddings with positional embeddings improves performance slightly.\\n\\n**Main Takeaways:**\\n\\n* For optimal performance, use a large hidden network size (e.g., dmodel=1024, dff=4096) and a high number of attention heads (e.g., h=16).\\n* Keep the kernel sizes in the convolutional layers small (e.g., dk=32, dv=32) for best results.\\n* Employ appropriate regularization techniques (e.g., dropout, label smoothing) to enhance model generalization.\\n* Consider using positional embeddings instead of sinusoidal embeddings for a slight improvement in performance.',\n",
       "  '**Key Insights**\\n\\n- Discriminative training methods perform well on the WSJ 23 F1 task, with scores ranging from 88.3% to 91.7%.\\n- Semi-supervised training techniques can improve F1 scores by 1-2%, with the highest score of 92.7% achieved by a transformer model.\\n- Multi-task training also leads to high F1 scores, with a score of 93.0% obtained by the Luong et al. (2015) model.\\n- Generative training methods achieve the highest F1 score of 93.3%, as demonstrated by the Dyer et al. (2016) model.\\n\\n**Main Takeaways**\\n\\n- For the WSJ 23 F1 task, semi-supervised and multi-task training methods generally outperform discriminative training methods.\\n- Generative training methods achieve the best performance, suggesting that they may be more effective for syntactic parsing tasks.\\n- Transformer models perform well on all training types, indicating their versatility and effectiveness in natural language processing.',\n",
       "  '### Summary\\n\\nThis table presents a list of references related to neural machine translation.\\n\\n### Key Insights\\n\\n* The first reference ([2]) by Bahdanau et al. (2014) introduced a neural machine translation model that jointly learns to align and translate text. This model was a significant advancement in the field of neural machine translation.\\n* The second reference ([3]) by Britz et al. (2017) describes a large-scale exploration of neural machine translation models. This study evaluated the performance of different model architectures and training techniques on a variety of language pairs.\\n\\n### Main Takeaways\\n\\n* Neural machine translation models have made significant progress in recent years, and they are now able to achieve state-of-the-art performance on a variety of language pairs.\\n* The joint learning of alignment and translation is a key factor in the success of neural machine translation models.\\n* Large-scale exploration of neural machine translation models is important for understanding the strengths and weaknesses of different model architectures and training techniques.',\n",
       "  '**Summary**\\n\\nThe provided table contains only padding data, with every cell containing the string \"&lt;pad&gt;\". This indicates that the table currently has no meaningful information or data.\\n\\n**Key Insights**\\n\\n* The table is currently empty and contains no meaningful data.\\n* The padding data suggests that the table is either in a placeholder state or has been cleared.\\n\\n**Main Takeaways**\\n\\n* No actionable insights or conclusions can be drawn from the current table data.\\n* Additional data or information needs to be added to the table before analysis can be performed.'],\n",
       " 'image_summaries': ['This image is a diagram illustrating the architecture of a Transformer decoder, a crucial component in many modern neural machine translation (NMT) and natural language processing (NLP) models.  Let\\'s break down the visual elements and their significance:\\n\\n**Main Visual Elements:**\\n\\nThe diagram primarily uses boxes and arrows to depict the flow of information through the decoder.  The boxes represent different processing layers or components, and the arrows indicate the direction of data movement. The color-coding of the boxes helps visually categorize the different layer types.  Key elements include:\\n\\n* **Input Embedding:** This block represents the initial conversion of input words or tokens into numerical vectors that the model can understand.\\n\\n* **Positional Encoding:**  This circular element adds positional information to the input embeddings. Since transformers process sequences in parallel, this is essential for conveying word order which is lost in parallel processing.\\n\\n* **Masked Multi-Head Attention:**  This is a core component of the Transformer architecture.  The \"masked\" qualifier signifies that this attention mechanism only attends to previous positions in the input sequence during training, preventing the model from \"peeking\" at future tokens.  \"Multi-Head\" refers to the use of multiple attention mechanisms running in parallel, each focusing on different aspects of the input.\\n\\n* **Multi-Head Attention:** Similar to masked multi-head attention, but without the masking; this is used in encoder-decoder models to allow the decoder to attend to all the encoder outputs.\\n\\n* **Add & Norm:** These blocks represent the application of residual connections (adding the input to the output of a layer) and layer normalization (normalizing the activations to improve training stability and performance).\\n\\n* **Feed Forward:** This represents a fully connected feed-forward neural network applied independently to each position in the sequence.\\n\\n* **Output Embedding:** This block converts the decoder\\'s internal representation back into a format suitable for generating output words or tokens.\\n\\n* **Linear:** A fully connected layer.\\n\\n* **Softmax:** A function that converts the linear layer\\'s output into a probability distribution over the vocabulary, allowing the model to select the most likely next word.\\n\\n* **Output Probabilities:** The final output of the decoder, showing the probabilities of different words being the next word in the sequence.\\n\\n\\n**Key Components, Diagrams, and Graphs:**\\n\\nThe image is a block diagram, not a graph in the mathematical sense.  The key components are the layers of the Transformer decoder, arranged vertically to show the sequential nature of processing within each layer.  The horizontal connections represent the attention mechanisms connecting different positions in the sequence. The `Nx` labels indicate that the layers are repeated N times, creating a stack of encoder layers to allow for deeper processing.\\n\\n\\n**Context and Purpose:**\\n\\nThe purpose of the image is to visually explain the architecture of a Transformer decoder.  It\\'s intended for individuals familiar with neural network architectures, providing a high-level overview of the model\\'s structure and information flow.  It\\'s commonly used in research papers, educational materials, and presentations related to NLP and machine translation.\\n\\n\\n**Technical Insights:**\\n\\n* **Self-Attention:** The multi-head attention mechanisms are crucial because they allow the model to weigh the importance of different words in the input sequence when generating output.\\n\\n* **Parallel Processing:**  The Transformer architecture\\'s reliance on self-attention enables parallel processing of the input sequence, making it significantly faster than recurrent neural networks (RNNs) for long sequences.\\n\\n* **Layer Normalization:**  The inclusion of layer normalization contributes to the model\\'s ability to handle long sequences and train effectively.\\n\\n* **Residual Connections:** Residual connections help prevent vanishing gradients during training, allowing for deeper networks with improved performance.\\n\\nIn summary, the image provides a clear and concise visual representation of a Transformer decoder\\'s architecture, highlighting its key components and their interactions.  The diagram effectively communicates the complex functionality of this important NLP model.\\n']}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanu\\AppData\\Local\\Temp\\ipykernel_4780\\1982579213.py:2: LangChainDeprecationWarning: Default values for HuggingFaceBgeEmbeddings.model_name were deprecated in LangChain 0.2.5 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceBgeEmbeddings constructor instead.\n",
      "  retriever = extractor.create_multi_vector_retriever(summaries)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dhanu\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b30d9ff590b4422a20b78483a6478c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanu\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dhanu\\.cache\\huggingface\\hub\\models--BAAI--bge-large-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6b9c527b894a3882d7563812249170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab2613c5aec42fb9c678e7c7ce24538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/90.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464d3f986c1b4ec2aac753ca8417a93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0db784f2344c01ac64cebf70b527e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6fbd93c6234884b3e68a32e6657f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f884c0d54447e4b351d7e68e875021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff320dd43694dbd82d21b3ba284afcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c63362690f461eaf487833218d3729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00247a1dc946436aaafa11e26a4b9259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f910e59c4154ff5bcc6f5a0a2738401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanu\\AppData\\Local\\Temp\\ipykernel_4780\\1776779983.py:163: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# Create multi-vector retriever\n",
    "retriever = extractor.create_multi_vector_retriever(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RAG chain\n",
    "rag_chain = extractor.build_rag_chain(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "        \"What is the attention mechanism?\",\n",
    "        \"Describe the key components of transformers\",\n",
    "        \"Explain multihead attention\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the attention mechanism?\n",
      "Response: The context does not provide information on the attention mechanism, so I cannot answer this question.\n",
      "\n",
      "Question: Describe the key components of transformers\n",
      "Response: **Key Components of Transformers**\n",
      "\n",
      "* **Core:**\n",
      "    * Made of laminated silicon steel to minimize eddy current and hysteresis losses\n",
      "    * Typically comprises two or more limbs\n",
      "* **Primary Winding:**\n",
      "    * Layer of insulated copper wire wound around one limb of the core\n",
      "    * Receives electrical power from the source\n",
      "* **Secondary Winding:**\n",
      "    * Layer of insulated copper wire wound around another limb of the core\n",
      "    * Provides output electrical power\n",
      "* **Magnetic Shunt:**\n",
      "    * Additional winding or magnetic material used to divert excess magnetic flux\n",
      "* **Tank:**\n",
      "    * Enclosed structure containing the core and windings\n",
      "    * Can be filled with oil, nitrogen, or air for insulation and cooling\n",
      "* **Bushings:**\n",
      "    * High-voltage insulators that connect the windings to the outside world\n",
      "* **Tap Changer:**\n",
      "    * Mechanism that allows for adjustment of the voltage ratio between the windings\n",
      "* **Cooling System:**\n",
      "    * Components (e.g., heat sinks, fans) designed to dissipate heat generated during operation\n",
      "* **Conservator:**\n",
      "    * External reservoir that compensates for changes in oil volume due to temperature variations\n",
      "* **Breather:**\n",
      "    * Allows for exchange of air while preventing moisture ingress\n",
      "\n",
      "Question: Explain multihead attention\n",
      "Response: **Multihead Attention**\n",
      "\n",
      "Multihead attention is a mechanism widely used in transformer neural networks, which are state-of-the-art models for various language-related tasks. It allows the model to attend to different parts of a sequence simultaneously, capturing multiple perspectives and relationships within the data. Here's a step-by-step explanation of multihead attention:\n",
      "\n",
      "**1. Input Embeddings:**\n",
      "\n",
      "Before applying multihead attention, the input sequence of tokens (words or subwords) is typically converted into vector representations, known as embeddings. These embeddings can capture the semantic and syntactic information associated with each token.\n",
      "\n",
      "**2. Projections:**\n",
      "\n",
      "For multihead attention, each input embedding is projected onto multiple different subspaces using trainable matrices, known as projection matrices. These matrices create different linear combinations of the embedding features, effectively generating multiple sets of \"heads.\"\n",
      "\n",
      "**3. Scaled Dot-Product Attention:**\n",
      "\n",
      "For each head, the projected embeddings are passed through a scaled dot-product attention mechanism:\n",
      "\n",
      "- **Query (Q):** The projected embeddings are used as query vectors to identify relevant parts of the sequence.\n",
      "- **Keys (K):** Another set of projected embeddings is used as key vectors to encode the content of the sequence.\n",
      "- **Values (V):** A third set of projected embeddings represents the values that will be used to aggregate information.\n",
      "- **Dot Product:** The query and key vectors are computed to obtain a similarity matrix, measuring the compatibility between each query and key.\n",
      "- **Scaling:** To prevent large values from dominating the attention, the dot product is scaled by the square root of the dimension of the key vectors.\n",
      "- **Softmax:** A softmax function is applied to the scaled dot product to obtain a distribution of attention weights, indicating the importance of each key in relation to the query.\n",
      "- **Value Context:** The attention weights are then multiplied by the corresponding value vectors to obtain a weighted sum of values, creating a new vector that represents the context relevant to the query.\n",
      "\n",
      "**4. Concatenation:**\n",
      "\n",
      "The output of each attention head, representing a different perspective on the sequence, is concatenated together.\n",
      "\n",
      "**5. Final Projection:**\n",
      "\n",
      "The concatenated vectors are further projected using another trainable matrix to obtain the final output of the multihead attention layer.\n",
      "\n",
      "**Benefits of Multihead Attention:**\n",
      "\n",
      "- **Capturing Different Relationships:** Multihead attention allows the model to identify multiple and diverse relationships within the sequence, as it operates on different subspaces simultaneously.\n",
      "- **Improved Representational Power:** By combining multiple perspectives, multihead attention enhances the representational power of the model, leading to better performance on complex tasks.\n",
      "- **Parallelism:** The different heads can be computed in parallel, making multihead attention efficient to train on large datasets.\n",
      "- **Regularization:** By using multiple heads, multihead attention encourages the model to learn robust and diverse features, reducing overfitting.\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    response = rag_chain(question)\n",
    "    print(\"Response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
